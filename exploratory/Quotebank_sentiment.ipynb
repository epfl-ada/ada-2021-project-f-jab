{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3930df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import norm\n",
    "\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# afinn\n",
    "from afinn import Afinn \n",
    "\n",
    "# transformer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64678412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fd93b",
   "metadata": {},
   "source": [
    "# Sentiment score investigations\n",
    "This notebook investigates the properties of two different sentiment score approaches; 1) the lexicon- and rule-based VADER approach and 2) a tranformer based approach using a pretrained BERT to model sentiment of quotes. A third score - the naïve AFINN lexicon-based approach - is also provided but is deemed to simple from the beginning and serve as a possible comparison (if wanted). \n",
    "\n",
    "### Initial analyses\n",
    "We will start by investigating three different approaches for adding sentiment scores\n",
    "to quotes. One uses the simpler dictionary-based sentiment ranking system of\n",
    "the [AFINN lexicon](http://www2.imm.dtu.dk/pubdb/edoc/imm6006.pdf). Another is the [VADER sentiment module](https://github.com/cjhutto/vaderSentiment) that was used in the tutorial on text handling. This is lexicon- and rule-based and tuned to sentiments occuring in textual data from socia media and has the advantage to AFINN that it includes the negative, neutral, positive as well as the compound (combined and scaled between -1 and 1) sentiment scores of a string whereas this should be manually handled with AFINN. The last approach uses\n",
    "the nowadays hyped transformers, where a pretrained transformer\n",
    "from [HuggingFace](https://github.com/huggingface/transformers) determined to use the distilled BERT model that is unsenstive to the case of words (the default model). We choose to stay with the default pretrained model as the distillation makes it faster and the unsensitiveness to casing as we want to examine quotes and not online textual expressions as tweets. With the latter we mean that the casing is redundant to incorporate when dealing with spoken text compared to written text expressions - as an example the case sensitive model would be of interest to use for tweets where \"FUCK\" and \"fuck\" should definitely be related to different scores. The default model the model `distilbert-base-uncased` and we would like to refer to the [paper accompanying the model](https://arxiv.org/abs/1910.01108) for more technical details.\n",
    "\n",
    "In general we put more weight on the VADER and Transformer approaches as they are less naïve but we keep the AFINN as an option that might be a \"funny\" feature for interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7765bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Afinn\n",
    "#!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb5e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "afn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3a7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32aefcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8dcb3",
   "metadata": {},
   "source": [
    "We initially test the approaches on a couple of examples to get an idea of how and whether they work or not. A helper function for the AFINN and VADER objects is used for similar output as the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b86f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_aux(score, threshold=0):\n",
    "    \"\"\"Helper function for outputting similar output as the transformer.\"\"\"\n",
    "    \n",
    "    if score < threshold:\n",
    "        prediction_dict = [{'label': 'NEGATIVE', 'score': score}]\n",
    "    else:\n",
    "        prediction_dict = [{'label': 'POSITIVE', 'score': score}]\n",
    "        \n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf7228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Fuck you fucking dumb BERT model, you dumbass\":\n",
      " Transformer: \t [{'label': 'NEGATIVE', 'score': 0.9992602467536926}]\n",
      " AFINN:\t\t [{'label': 'NEGATIVE', 'score': -14.0}]\n",
      " VADER: \t [{'label': 'NEGATIVE', 'score': -0.8932}]\n",
      "\n",
      "\"I love my ADA group - they're the best! <3\":\n",
      " Transformer: \t [{'label': 'POSITIVE', 'score': 0.9998805522918701}]\n",
      " AFINN:\t\t [{'label': 'POSITIVE', 'score': 6.0}]\n",
      " VADER: \t [{'label': 'POSITIVE', 'score': 0.9117}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_sentence = \"Fuck you fucking dumb BERT model, you dumbass\"\n",
    "print(f'\"{negative_sentence}\":\\n Transformer: \\t {classifier(negative_sentence)}\\n AFINN:\\t\\t {output_aux(afn.score(negative_sentence))}\\n VADER: \\t {output_aux(vaderAnalyzer.polarity_scores(negative_sentence)[\"compound\"])}\\n')\n",
    "\n",
    "positive_sentence = \"I love my ADA group - they're the best! <3\"\n",
    "print(f'\"{positive_sentence}\":\\n Transformer: \\t {classifier(positive_sentence)}\\n AFINN:\\t\\t {output_aux(afn.score(positive_sentence))}\\n VADER: \\t {output_aux(vaderAnalyzer.polarity_scores(positive_sentence)[\"compound\"])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d89962",
   "metadata": {},
   "source": [
    "Based on this single (however super representative!!!) example we see that all three approaches work but that the output scores are quite differently scaled. This is due to the fact that the transformer has a softmax layer in the end and therefore outputs the probability of the most probable label - which is either \"POSITIVE\" or \"NEGATIVE\". On the other hand AFINN accumulates a the score of both positive and negative words to get a single score. The AFINN score can thus strongly depend on the length of a sentence and we would need to introduce a normalization term. The VADER scores are as mentioned pre-scaled and normalized in the range between -1 and 1 in the module implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953e8844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<3\":\n",
      " Transformer: \t [{'label': 'POSITIVE', 'score': 0.979346513748169}]\n",
      " AFINN:\t\t [{'label': 'POSITIVE', 'score': 0.0}]\n",
      " VADER: \t [{'label': 'POSITIVE', 'score': 0.4404}]\n",
      "\n",
      "\":-)\":\n",
      " Transformer: \t [{'label': 'POSITIVE', 'score': 0.5607805848121643}]\n",
      " AFINN:\t\t [{'label': 'POSITIVE', 'score': 0.0}]\n",
      " VADER: \t [{'label': 'POSITIVE', 'score': 0.3182}]\n",
      "\n",
      "\":-(\":\n",
      " Transformer: \t [{'label': 'NEGATIVE', 'score': 0.87599778175354}]\n",
      " AFINN:\t\t [{'label': 'POSITIVE', 'score': 0.0}]\n",
      " VADER: \t [{'label': 'NEGATIVE', 'score': -0.3612}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "symbols = ['<3', ':-)', ':-(']\n",
    "\n",
    "for sym in symbols:\n",
    "    print(f'\"{sym}\":\\n Transformer: \\t {classifier(sym)}\\n AFINN:\\t\\t {output_aux(afn.score(sym))}\\n VADER: \\t {output_aux(vaderAnalyzer.polarity_scores(sym)[\"compound\"])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7586404",
   "metadata": {},
   "source": [
    "As seen above some of the approaches differ when examining symbols like smileys and hearts. Here, the manually defined AFINN lexicon can not encounter such tokens, where the transformer on the contrary has learned to encounter these and in fact do quite a good job (except for the 'positive' smiley). The rule-based lexicon of VADER has indeed also learned the mapping of such smileys and symbols. Even though symbols like these are not immediately associated with quotes - as quotes are text capturing spoken language - it might come in as a good feature that the transformer and the VADER approaches are more complex and understand the local sentiment of subtokens better than AFINN. We therefore already disregard AFINN as the better approach but keep it as a possible feature for interactive plots as previously mentioned. We will investigate the other two a bit more and apply all three scores to a new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81686037",
   "metadata": {},
   "source": [
    "### Applying sentiment scores to the data (skipable)\n",
    "This section can be skipped if it has previously been executed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b4595",
   "metadata": {},
   "source": [
    "First, we load the data that was previously processed. The pickled file was created with pandas 1.3.3. and has to be loaded with the same pandas version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27219f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>movie</th>\n",
       "      <th>shared_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Ferguson like Mockingjay?</td>\n",
       "      <td>Laci Green</td>\n",
       "      <td>[Q16843606]</td>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Laci Green, 0.9013], [None, 0.0987]]</td>\n",
       "      <td>[http://www.dailykos.com/story/2015/11/15/1450...</td>\n",
       "      <td>The Hunger Games: Mockingjay - Part 2</td>\n",
       "      <td>1412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to clarify my interview on the `Charlie...</td>\n",
       "      <td>George Lucas</td>\n",
       "      <td>[Q1507803, Q38222]</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>7</td>\n",
       "      <td>[[George Lucas, 0.5327], [None, 0.4248], [Char...</td>\n",
       "      <td>[http://www.escapistmagazine.com/news/view/165...</td>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Daredevil joining the Avengers for Infinity...</td>\n",
       "      <td>Scott Davis</td>\n",
       "      <td>[Q16195496, Q18202175, Q7436225, Q7436228, Q12...</td>\n",
       "      <td>2015-12-10</td>\n",
       "      <td>2</td>\n",
       "      <td>[[None, 0.4806], [Scott Davis, 0.4017], [Antho...</td>\n",
       "      <td>[http://www.flickeringmyth.com/2015/12/is-dare...</td>\n",
       "      <td>Avengers: Age of Ultron</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They were saying, `Well, since when has Star W...</td>\n",
       "      <td>J.J. Abrams</td>\n",
       "      <td>[Q188137]</td>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>1</td>\n",
       "      <td>[[J.J. Abrams, 0.5868], [None, 0.2584], [Lupit...</td>\n",
       "      <td>[http://rssfeeds.usatoday.com/~/129385923/0/us...</td>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You meet new characters and you learn about Ha...</td>\n",
       "      <td>Kevin Feige</td>\n",
       "      <td>[Q515161]</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kevin Feige, 0.9108], [None, 0.0782], [Scott...</td>\n",
       "      <td>[http://www.digitaltrends.com/movies/ant-man-m...</td>\n",
       "      <td>Avengers: Age of Ultron</td>\n",
       "      <td>7091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           quotation       speaker  \\\n",
       "0                       Is Ferguson like Mockingjay?    Laci Green   \n",
       "1  I want to clarify my interview on the `Charlie...  George Lucas   \n",
       "2  Is Daredevil joining the Avengers for Infinity...   Scott Davis   \n",
       "3  They were saying, `Well, since when has Star W...   J.J. Abrams   \n",
       "4  You meet new characters and you learn about Ha...   Kevin Feige   \n",
       "\n",
       "                                                qids        date  \\\n",
       "0                                        [Q16843606]  2015-11-15   \n",
       "1                                 [Q1507803, Q38222]  2015-12-31   \n",
       "2  [Q16195496, Q18202175, Q7436225, Q7436228, Q12...  2015-12-10   \n",
       "3                                          [Q188137]  2015-12-21   \n",
       "4                                          [Q515161]  2015-05-06   \n",
       "\n",
       "   numOccurrences                                             probas  \\\n",
       "0               1             [[Laci Green, 0.9013], [None, 0.0987]]   \n",
       "1               7  [[George Lucas, 0.5327], [None, 0.4248], [Char...   \n",
       "2               2  [[None, 0.4806], [Scott Davis, 0.4017], [Antho...   \n",
       "3               1  [[J.J. Abrams, 0.5868], [None, 0.2584], [Lupit...   \n",
       "4               1  [[Kevin Feige, 0.9108], [None, 0.0782], [Scott...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://www.dailykos.com/story/2015/11/15/1450...   \n",
       "1  [http://www.escapistmagazine.com/news/view/165...   \n",
       "2  [http://www.flickeringmyth.com/2015/12/is-dare...   \n",
       "3  [http://rssfeeds.usatoday.com/~/129385923/0/us...   \n",
       "4  [http://www.digitaltrends.com/movies/ant-man-m...   \n",
       "\n",
       "                                        movie  shared_ID  \n",
       "0       The Hunger Games: Mockingjay - Part 2       1412  \n",
       "1  Star Wars: Episode VII - The Force Awakens        700  \n",
       "2                     Avengers: Age of Ultron        999  \n",
       "3  Star Wars: Episode VII - The Force Awakens       1926  \n",
       "4                     Avengers: Age of Ultron       7091  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying filename and directory\n",
    "data_dir = os.getcwd() + os.sep + 'data'\n",
    "filepath = rf\"{data_dir}{os.sep}Quotebank_processed.pkl\"\n",
    "\n",
    "df = pd.read_pickle(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ce829",
   "metadata": {},
   "source": [
    "Now we compute the sentiment scores with AFINN score, VADER score and the BERT transformer based score. We do two things here; first we redefine the auxiliary function used to change the output format such that it normalizes the sentiment by the length of a quote (defined by number of words splitted at whitespaces) when using AFINN and second we are aware that the transformer is limited to a certain input sequence length and add a NaN-value for a quote if it is longer than this maximum sequence length. Later we will try to scale the different scores such that they are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ce0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afn_aux(quote):\n",
    "    \"\"\"Helper function for outputting similar output as the transformer.\"\"\"\n",
    "    \n",
    "    score = afn.score(quote) / len(quote.split(\" \"))\n",
    "\n",
    "    if score < 0:\n",
    "        prediction_dict = [{'label': 'NEGATIVE', 'score': score}]\n",
    "    else:\n",
    "        prediction_dict = [{'label': 'POSITIVE', 'score': score}]\n",
    "        \n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5c89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78660ea9a37248ada844a7ecfd01a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiments_AFINN = [afn_aux(quote) for quote in tqdm(df.quotation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b74366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57453ee57cbc4908ae28df1c5461827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiments_VADER = [output_aux(vaderAnalyzer.polarity_scores(quote)['compound']) for quote in tqdm(df.quotation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e514b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e768e1cd27e47d29c575ed769f218b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# the model is limited to handling sequences of a prespecified length.\n",
    "# We handle this by using the try-except statement and add a nan-value where the \n",
    "# BERT can't handle the input quote.\n",
    "sentiments_BERT = []\n",
    "for quote in tqdm(df.quotation):\n",
    "    try:\n",
    "        BERT_score = classifier(quote)\n",
    "        sentiments_BERT.append(BERT_score)\n",
    "    except RuntimeError:\n",
    "        sentiments_BERT.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2beed",
   "metadata": {},
   "source": [
    "We now extract the label and score from the sentiment scores dictionaries and create individual attributes for these for each sentiment approach. This is done for easier access in future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb67ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_label, AFINN_score = [], []\n",
    "VADER_label, VADER_score = [], []\n",
    "BERT_label, BERT_score = [], []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    label, score = sentiments_AFINN[i][0].values()\n",
    "    AFINN_label.append(label), AFINN_score.append(score)\n",
    "    \n",
    "    label, score = sentiments_VADER[i][0].values()\n",
    "    VADER_label.append(label), VADER_score.append(score)\n",
    "    \n",
    "    label, score = sentiments_BERT[i][0].values()\n",
    "    BERT_label.append(label), BERT_score.append(score)\n",
    "    \n",
    "\n",
    "# update sentiment attributes\n",
    "df['AFINN_label'] = AFINN_label\n",
    "df['AFINN_score'] = AFINN_score\n",
    "df['VADER_label'] = VADER_label\n",
    "df['VADER_score'] = VADER_score\n",
    "df['BERT_label'] = BERT_label\n",
    "df['BERT_score'] = BERT_score\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f9b82",
   "metadata": {},
   "source": [
    "We check how many NaN values we created by use of the transformer and remove them. The justification for this is mainly that we do not have time for training a new model and prefer using a pretrained one that has already shown a prominent performance and that it seems plausible that (super) long quotes are erroneous data that has not been removed in the Quotebank approach of scraping the quotes from newspaper articles. Below we print out 3 random of the 88 quotes that were too long for the transformer to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54101d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of NaN-values in BERT sentiment score: {df.isnull().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04097d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_quotes = df[df.isnull().any(axis=1)]\n",
    "\n",
    "random_ids = np.random.randint(0, len(NaN_quotes), size=3)\n",
    "for random_id in random_ids:\n",
    "    print(f\"{NaN_quotes.quotation.iloc[random_id]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a408f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[-df.isnull().any(axis=1)]\n",
    "df.index = np.arange(len(df))\n",
    "\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b364df",
   "metadata": {},
   "source": [
    "### Investigating the sentiment attributes (specific)\n",
    "\n",
    "We now want to investigate the distribution of the sentiment scores for each of the approaches. We will invest all of the three but with an emphasis on VADER and the Transformer-based approach. We start by extracting the scores and label from the dictionary structure that the scores are stored and vizualising the distribution of the BERT and VADER sentiments before re-scaling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aadfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentiment scores and labels from the \n",
    "VADER_scores = df.VADER_score\n",
    "VADER_labels = df.VADER_label\n",
    "\n",
    "BERT_scores = df.BERT_score \n",
    "BERT_labels = df.BERT_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "BERT_scores.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax1.set_xlim(0,1)\n",
    "ax1.set_ylabel(\"count\")\n",
    "ax1.xaxis.grid(False)\n",
    "ax1.set_title(\"Distribution BERT scores (certainty)\")\n",
    "\n",
    "ax2 = plt.subplot(122, sharey=ax1)\n",
    "VADER_scores.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax2.set_ylabel(\"count\")\n",
    "ax2.xaxis.grid(False)\n",
    "ax2.set_title(\"Distribution of VADER scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7aa7a",
   "metadata": {},
   "source": [
    "From these plots we make some interesting explorations: as the BERT scores are softmax outputs we see that the BERT is extremely certain in it's predictions, meaning that the model tends to be more than 90-95% percent certain that the label it is predicting is correct. Furthermore, we are confirmed that we need to change the score for the BERT predictions such that it distinguishes negative from positive scores if it is to be compared with the VADER scores. On the other hand when we look at the VADER scores, it is clearly seen that the sentiment is pretty evenly distributed with exceptions around -0.5 and 0. The spike at 0 reveals that the VADER scores contain additional properties than the BERT, as it can be used to label a sentence neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823269e3",
   "metadata": {},
   "source": [
    "To deal with the issues of the BERT scores we start by expressing all the scores as the models probability of labelling the given score as \"POSITIVE\". In this way the scores are probabilities of the POSITIVE class that can take values from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2634ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the negative labels as probability of being positive\n",
    "df['positive_BERT_score'] = df.BERT_score\n",
    "df.loc[df.BERT_label == 'NEGATIVE', 'positive_BERT_score'] = 1 - BERT_scores[BERT_labels == 'NEGATIVE'] #[output_aux(score, threshold=0.5) for score in BERT_scores]\n",
    "\n",
    "posBERT_scores = df['positive_BERT_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd50c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "posBERT_scores.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax1.set_xlim(0,1)\n",
    "ax1.set_ylabel(\"count\")\n",
    "ax1.set_title(\"Distribution of POSITIVE BERT scores (certainty)\")\n",
    "ax1.xaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b05bed",
   "metadata": {},
   "source": [
    "What seems to be the situation for the BERT scores is that there are more POSITIVE labels that the BERT is super certain about. Before investigating the relation between positive and negative labelled classes we want to rescale the BERT scores to the same scale as the VADER scores, such that they are comparable. Since the BERT scores are outputs of a softmax which generalizes to the sigmoid in the binary case (which we have here) we revert the BERT scores (reverted sigmoid) and scale them to the range -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd300a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "revertedBERT = np.log(posBERT_scores / (1-posBERT_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a930f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scaledReverted_BERT_score'] = 2*(revertedBERT-np.min(revertedBERT)) / (np.max(revertedBERT) - np.min(revertedBERT)) - 1\n",
    "scaledRevertedBERT = df['scaledReverted_BERT_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12018b6",
   "metadata": {},
   "source": [
    "We now have all sentiment scores we are interested in and save them in a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loaded DataFrame as pickle for faster loading time in the future\n",
    "filename = f\"Quotebank_sentiment\"\n",
    "df.to_pickle(rf\"{data_dir}{os.sep}{filename}.pkl\")\n",
    "\n",
    "print(f\"Dataframe was pickled and saved to directory:\\n{data_dir} \\n\")\n",
    "print(f\"Shape of dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "scaledRevertedBERT.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax1.set_ylabel(\"count\")\n",
    "ax1.xaxis.grid(False)\n",
    "ax1.set_title(\"Distribution of scaled reverted POSITIVE BERT scores\")\n",
    "\n",
    "ax2 = plt.subplot(122, sharey=ax1)\n",
    "VADER_scores.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax2.set_ylabel(\"count\")\n",
    "ax2.xaxis.grid(False)\n",
    "ax2.set_title(\"Distribution of VADER scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c9040",
   "metadata": {},
   "source": [
    "So we see that the scores are now comparable and actually look somewhat similar. We still see that very few of the BERT scores are classified as neutral and tend to have the density in a negative and a positive cluster. The two approaches sentiment prediction especially resemble eachother when comparing with the normalized AFINN score in the plot below. We do not scale this to a range of -1 to 1 as it seems to be normally distributed and would therefore be heavily influenced by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_scores = df.AFINN_score \n",
    "AFINN_labels = df.AFINN_label\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "AFINN_scores.hist(edgecolor='white', linewidth=1, bins=51, alpha=0.65)\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee5c4c",
   "metadata": {},
   "source": [
    "We now want to examining the mean scores and distribution of labels across the classes \"POSITIVE\" and \"NEGATIVE\" to get a sense of how each approach characterizes the quotes. In the below plot we visualize the mean sentiment scores for each approach with a 95% confidence interval as well as the distribution of POSITIVE vs. NEGATIVE labels categorized by each approach. Be aware that the AFINN scores do not make sense to directly compare to the other ones in regards to the continuous score as it is on a different scale, for which reason it is left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215916d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(16,6),sharey='row')\n",
    "\n",
    "attributes = [\"AFINN\", \"VADER\", \"BERT\"]\n",
    "x = [0,1]    \n",
    "\n",
    "for i, att in enumerate(attributes):\n",
    "    if att == 'BERT':\n",
    "        sent_scores = df[f'scaledReverted_BERT_score']\n",
    "    else:\n",
    "        sent_scores = df[f'{att}_score']\n",
    "\n",
    "    sent_labels = df[f'{att}_label'] \n",
    "    ids_without_neutral = -np.logical_and(sent_labels==\"POSITIVE\", sent_scores==0)\n",
    "    \n",
    "    labels, counts = np.unique(sent_labels[ids_without_neutral], return_counts=True)\n",
    "    temp = pd.DataFrame({'score': sent_scores[ids_without_neutral], 'labels': sent_labels[ids_without_neutral]})\n",
    "    classes = temp.groupby(by='labels')\n",
    "    \n",
    "    err = (1.96*classes.std()/np.sqrt(sum(counts))).T.to_numpy()[0]\n",
    "    \n",
    "    ax[0, i].set_xticks(x)       \n",
    "    if i != 0:\n",
    "        ax[0, i].bar(x, abs(classes.mean().T.to_numpy()[0]), yerr=abs(err), color=['C0', 'C1'], alpha=0.65)\n",
    "        ax[0, i].set_xticklabels(labels)\n",
    "        ax[0, i].set_title(f\"Mean {att} score with SEM error\")\n",
    "        ax[0, i].set_ylim(0,1)\n",
    "        ax[0, i].xaxis.grid(False)     \n",
    "        ax[0, i].set_ylabel(\"percentage\")\n",
    "        \n",
    "    ax[1, i].bar(x, counts, color=['C0', 'C1'], alpha=0.65)\n",
    "    ax[1, i].set_xticks(x)\n",
    "    ax[1, i].set_xticklabels(labels)\n",
    "    ax[1, i].set_title(f\"{att} label dist.\")\n",
    "    ax[1, i].xaxis.grid(False)\n",
    "    ax[1, i].set_ylabel(\"count\")\n",
    "\n",
    "\n",
    "plt.suptitle(\"Distribution of positive/negative labels for each approach (neutral sent. = 0 removed)\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ccc3",
   "metadata": {},
   "source": [
    "The first row of the above subplot reveals that the scores obtained with VADER and BERT are similar. The second row reveals that there is an overweight of the POSITIVE label no matter which approach is used - also when the sentiment scores equal to 0 are removed. Furthermore, this reveals the first thing about the sentiment of quotes about movies - namely that there is a slight overweigth of positively associated quotes. Lets investigate the labels of one approach when conditioning on the other (between VADER and BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=((16,8)), sharey=True)\n",
    "x = [0, 1]\n",
    "\n",
    "helper_dict = {\"sent1\": [(VADER_labels, \"VADER\"), (VADER_labels, \"VADER\"), (BERT_labels, \"BERT\"), (BERT_labels, \"BERT\")], \n",
    "              \"sent2\": [(BERT_labels, \"BERT\"), (BERT_labels, \"BERT\"), (VADER_labels, \"VADER\"), (VADER_labels, \"VADER\")],\n",
    "              \"category\": [\"POSITIVE\", \"NEGATIVE\", \"POSITIVE\", \"NEGATIVE\"]}\n",
    "\n",
    "for i in range(4):\n",
    "    data = helper_dict[\"sent1\"][i][0][helper_dict[\"sent2\"][i][0] == helper_dict[\"category\"][i]]\n",
    "    labels, counts = np.unique(data, return_counts=True)\n",
    "    \n",
    "\n",
    "    ax[i%2, i//2].bar(x, counts/np.sum(counts), color=['C0', 'C1'], alpha=0.65)\n",
    "    ax[i%2, i//2].set_xticks(x)\n",
    "    ax[i%2, i//2].set_xticklabels(labels)\n",
    "    ax[i%2, i//2].set_ylim(0, 1)    \n",
    "    ax[i%2, i//2].set_title(f\"{helper_dict['sent1'][i][1]} labels when {helper_dict['sent2'][i][1]} is conditioned as {helper_dict['category'][i]}\")\n",
    "    ax[i%2, i//2].set_ylabel(\"percentage\")\n",
    "        \n",
    "plt.suptitle(\"Differences in labelling between BERT and VADER\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df1a8a",
   "metadata": {},
   "source": [
    "As we see on the first row of the above subplots the two approaches seem to agree in around 80% of the cases for both approaches when conditioning on the POSITIVE classifications from the other approach. Contrarily we see that both approaches do not agree as much on the NEGATIVE class - in fact there are even more POSITIVE labels using the VADER approach compared to the NEGATIVE labels when we condition and use the quotes from labelled as NEGATIVE by the transformer. We can get a sence of why this is the case when conditioning on the scores of the VADER approach. We investigate this in two manners; 1) by a comparative plot, and 2) by looking at concrete quotes when the labelling from BERT and VADER differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "def plotWithCondition(df, conditional_attribute, second_attribute, cond_bound=(-1,1), BERT_bound=(0, 1), movie=None):\n",
    "        \n",
    "    if movie != None:\n",
    "        movie_slice = (df.movie == movie)    \n",
    "        posBERT_scores = df[f'{second_attribute}_score'][movie_slice] #pd.Series([score_dict[0]['score'] for score_dict in df[second_attribute]])[movie_slice]\n",
    "        scaledRevertedBERT = df['scaledReverted_BERT_score'][movie_slice] #pd.Series([score_dict[0]['score'] for score_dict in df['scaledReverted_BERT']])[movie_slice]   \n",
    "        cond_scores = df[f'{conditional_attribute}_score'][movie_slice] #pd.Series([score_dict[0]['score'] for score_dict in df[conditional_attribute]])[movie_slice]\n",
    "        movie_title = movie\n",
    "\n",
    "    else:\n",
    "        posBERT_scores = df[f'{second_attribute}_score']# pd.Series([score_dict[0]['score'] for score_dict in df[second_attribute]])\n",
    "        scaledRevertedBERT = df['scaledReverted_BERT_score']#pd.Series([score_dict[0]['score'] for score_dict in df['scaledReverted_BERT']])   \n",
    "        cond_scores = df[f'{conditional_attribute}_score']#pd.Series([score_dict[0]['score'] for score_dict in df[conditional_attribute]])\n",
    "        movie_title = \"All movies\"\n",
    "    \n",
    "    condition = np.logical_and(cond_scores >= cond_bound[0], cond_scores < cond_bound[1])\n",
    "    cond_name = conditional_attribute\n",
    "    second_name = second_attribute\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,6))\n",
    "    fig.suptitle(f\"\\\"{movie_title}\\\" \", fontsize=20)\n",
    "\n",
    "    # plot of conditional attribute (VADER)\n",
    "    ax1 = plt.subplot(221)\n",
    "    _, bins, patches = ax1.hist(cond_scores, edgecolor='white', linewidth=1, bins=51, alpha=0.65, density=True)\n",
    "    ax1.set_title(f\"Distribution of {cond_name} scores \\n Condition (orange) = [{cond_bound[0], cond_bound[1]}]\")\n",
    "    ax1.set_ylabel('density', fontsize=12)\n",
    "    \n",
    "    bins_lower = np.where(bins >= cond_bound[0] - 0.02)[0]\n",
    "    bins_upper = np.where(bins <= cond_bound[1])[0]\n",
    "    \n",
    "    idxs = np.intersect1d(bins_lower, bins_upper)\n",
    "    for idx in idxs[:-1]:\n",
    "        patches[idx].set_facecolor('C1')\n",
    "      \n",
    "    # BERT probability of positive label plot\n",
    "    ax2 = plt.subplot(122)\n",
    "    _, bins, patches = ax2.hist(posBERT_scores[condition], edgecolor='white', linewidth=1, bins=51, alpha=0.65, density=True)\n",
    "    ax2.set_title(f\"BERT density of positive label\")\n",
    "    ax2.set_ylabel('density', fontsize=14)    \n",
    "\n",
    "    # plot of scaled and reverted BERT scores\n",
    "    ax3 = plt.subplot(223, sharex=ax1)\n",
    "    _, bins, patches = ax3.hist(scaledRevertedBERT[condition], edgecolor='white', linewidth=1, bins=51, alpha=0.65, density=True)\n",
    "    ax3.set_title(f\"Scaled, reverted {second_name} scores \\n Condition (lightblue) = [{BERT_bound[0], BERT_bound[1]}]\")\n",
    "    ax3.set_ylabel('density', fontsize=12)\n",
    "    ax3.set_xlim(-1.1,1.1)\n",
    "    \n",
    "    bins_lower = np.where(bins >= BERT_bound[0] - 0.02)[0]\n",
    "    bins_upper = np.where(bins <= BERT_bound[1])[0]\n",
    "    \n",
    "    idxs = np.intersect1d(bins_lower, bins_upper)\n",
    "    colour = to_rgb('skyblue')\n",
    "    #colour[2] = 0.5\n",
    "    for idx in idxs[:-1]:\n",
    "        patches[idx].set_facecolor(colour)\n",
    "    \n",
    "\n",
    "    ax1.xaxis.grid(False)\n",
    "    ax1.yaxis.grid(True)\n",
    "    ax2.xaxis.grid(False)\n",
    "    ax2.yaxis.grid(True)\n",
    "    ax3.xaxis.grid(False)\n",
    "    ax3.yaxis.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeadb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConditionQuotes(df, first_attribute, second_attribute, N=3, first_bound=(-1,-0.5), second_bound=(0.5, 1), movie=None):\n",
    "    \n",
    "    if movie != None:\n",
    "        movie_slice = (df.movie == movie)    \n",
    "        first_scores = df[f'{first_attribute}_score'][movie_slice] #pd.Series([score_dict[0]['score'] for score_dict in df[first_attribute]])[movie_slice]\n",
    "        second_scores = df[f'{second_attribute}_score'][movie_slice] #pd.Series([score_dict[0]['score'] for score_dict in df[second_attribute]])[movie_slice]\n",
    "        movie_title = movie\n",
    "\n",
    "    else: \n",
    "        first_scores = df[f'{first_attribute}_score'] #pd.Series([score_dict[0]['score'] for score_dict in df[first_attribute]])\n",
    "        second_scores = df[f'{second_attribute}_score'] #pd.Series([score_dict[0]['score'] for score_dict in df[second_attribute]])\n",
    "        movie_title = \"All movies\"\n",
    "    \n",
    "    first_condition = np.logical_and(first_scores >= first_bound[0], first_scores < first_bound[1])\n",
    "    second_condition = np.logical_and(second_scores >= second_bound[0], second_scores < second_bound[1])\n",
    "    \n",
    "    first_name = first_attribute\n",
    "    second_name = second_attribute\n",
    "    \n",
    "    both_conds = np.logical_and(first_condition, second_condition)\n",
    "    indeces_conds = first_scores.loc[both_conds].index\n",
    "    \n",
    "    random_quotes = np.random.permutation(indeces_conds)[:N]\n",
    "    #random_quotes = np.random.choice(indeces_conds)\n",
    "        \n",
    "    print(f\"Analyzing {N} randomly chosen quotes given that satisfy following conditions. \\nConditions: {first_name} = {first_bound}, {second_name} = {second_bound}\")\n",
    "    for i in range(N):\n",
    "        print(f\"\\nQuote w. index {random_quotes[i]}:\\n {df.loc[random_quotes[i]].quotation}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27577b",
   "metadata": {},
   "source": [
    "The following will first investigate the BERT scores when the VADER scores are either negative or strongly negative as well as give examples of quotes where the approaches disagree (in positive/negative labelling).\n",
    "\n",
    "Afterwards the same investigation will be done conditioning on positive or strongly positive VADER scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(-1,0), BERT_bound=(0,1))\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(-1, 0), second_bound=(0.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(-1,-0.75), BERT_bound=(0,1))\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(-1, -0.75), second_bound=(0.5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be8c2c",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "From the first of the two conditionings above we see that the BERT score are labelled negative almost as frequently as positive when looking at all quotes classified as negative by VADER (VADER score below 0). So the two approaches disagree on half of the labels classified as negative by VADER. From the second conditioning we see that this tendency reduces when looking at the strongly negative quotes classified by VADER where the BERT distribution gets more negative as well but still labels a large amount of the (negative by VADER) quotes as positive.\n",
    "\n",
    "From the randomly chosen quotes we realize that Star Wars or in general the word \"War\" occur a lot in the quotes - especially for the strongly negative quotes. We will get back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(0,1), BERT_bound=(-1,0))\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(0, 1), second_bound=(0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979adecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(0.75,1), BERT_bound=(-1,0))\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(0.75, 1), second_bound=(0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffe0f0",
   "metadata": {},
   "source": [
    "**Comments:** \n",
    "On the other hand the tendency is remarkably different when conditioning on the positively labelling by VADER of the scores. For positively labelled quotes by VADER, BERT generally agrees but labels a small amount of quotes as negative. However, when VADER predicts quotes strongly positive, almost all of the BERT predicitons are positive too.\n",
    "\n",
    "With regards to the random outputted examples it seems like \"War\" doesn't occur as much and that the quotes are in general longer than the mistakes when conditioning with negative VADER predicions. However, we are aware that the randomly outputted 5 quotes are not representative for all of the disagreeing labelled quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db1806",
   "metadata": {},
   "source": [
    "We will now look at the representation of the movies when checking the above conditions to investigate whether there is a trend between the disagreeing of the labelling of quotes and the movies they concern. The following function is used for analyzing this through horizontal bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplotWithCondition(df, VADER_bounds, BERT_bounds):\n",
    "\n",
    "    posBERT_scores = df['positive_BERT_score'] # pd.Series([score_dict[0]['score'] for score_dict in df[\"positive_BERT\"]])\n",
    "    VADER_scores = df['VADER_score'] #pd.Series([score_dict[0]['score'] for score_dict in df[\"sentiment_VADER\"]])\n",
    "    VADER_statement = np.logical_and(VADER_scores > VADER_bounds[0], VADER_scores < VADER_bounds[1])\n",
    "    BERT_statement = np.logical_and(posBERT_scores > BERT_bounds[0], posBERT_scores < BERT_bounds[1])\n",
    "\n",
    "    condition = np.logical_and(VADER_statement, BERT_statement)\n",
    "\n",
    "    title, count = np.unique(df.movie[condition], return_counts=True)\n",
    "    count, title = np.array(list(zip(*sorted(zip(count, title), reverse=False))))\n",
    "    count = np.array([int(number) for number in count])\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.title(f\"Quotes about movies \\n VADER condition: {VADER_bounds}, BERT condition: {BERT_bounds}\", fontsize=20)\n",
    "    plt.xlabel(\"count\", fontsize=12)\n",
    "    plt.barh(title, count, alpha=0.65)\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a07ca",
   "metadata": {},
   "source": [
    "We start by visualizing the unconditioned distribution of movies to have as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "barplotWithCondition(df, VADER_bounds=(-1, 1), BERT_bounds=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d5c19",
   "metadata": {},
   "source": [
    "Then we do the investigation with the same condition as in the previous investigation where we first look at the negatively labelled VADER predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "barplotWithCondition(df, VADER_bounds=(-1, 0), BERT_bounds=(0.5, 1))\n",
    "barplotWithCondition(df, VADER_bounds=(-1, -0.25), BERT_bounds=(0.5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d74e0f",
   "metadata": {},
   "source": [
    "Even though Star Wars and other movie titles that have a negative title are quite highly represented without conditioning on the data it still seems like conditioning on negative VADER labels exagerates this tendency. Also it seems like the above plot (conditioning on negative VADER) kind of ranks the scores from most negatively to most positively associated movie title. In other words negative VADER labels that are positively labelled by the transformer in general concern movies with negative titles. Even though this tendency sounds like a good feature of the VADER approach we do not prefer this to the BERT as we do not want the sentiment score to reflect the sentiment of the movie title but rather the context in which it appears.\n",
    "\n",
    "As we see below, there is not the same tendency for the disagreeing labelling when conditioning on positive VADER labels - it ressembles the original distribution more even though it is hard to say as we have a lot fewer data points with this conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f06482",
   "metadata": {},
   "outputs": [],
   "source": [
    "barplotWithCondition(df, VADER_bounds=(0, 1), BERT_bounds=(0, 0.5))\n",
    "barplotWithCondition(df, VADER_bounds=(0.75, 1), BERT_bounds=(0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582c090",
   "metadata": {},
   "source": [
    "### Discussion and comments on different sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e362952",
   "metadata": {},
   "source": [
    "Both approaches have pros and cons - here is a summary of them and an evaluation of which approach will be used as the main one for the future investigations.\n",
    "\n",
    "VADER seems to struggle with interpreting larger semantical structures in the sentence compared to the transformer based labelling which was seen through conditioning plots of score and movie title distributions and the qualitative analysis of quotes that has different labels with the two approaches. This conclusion is based on the fact that strongly negative scores from the VADER approach are labelled almost equally much as positive or negative with the BERT transformer approach. Through further conditioning investigations we saw that the quotes causing a difference in labelling between the two approaches tend to concern specific movies or the length of the quote (depending on whether positive or negative conditions were used). For instance, it seems like the thing that causes BERT to label many of the quotes labelled as strongly negative by VADER is that VADER does not catch the semanticly neutral sentiment of the word \"War\" when \"War\" is part of a movie title where BERT does not associate a negative sentiment with this. \n",
    "\n",
    "On the other hand, BERT predicts using the probability outcome of a softmax function and is in general very certain about its predictions. This mean that it does not make sense to infer a neutral sentiment assignment with this pretrained BERT transformer which is definetely a con of the transformer approach as not all quotes need to be associated with EITHER a positive or a negative score.\n",
    "\n",
    "Anyhow, we choose to continue with the transformer-based approach as it - as seen from initial analyses - is able to capture a more valid semantical understanding of quotes, which we weigh higher than the ability to do a neutral classification. We will however keep both but prioritize the BERT for further analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9097334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c10526a",
   "metadata": {},
   "source": [
    "### AD(A)itional feature\n",
    "\n",
    "Also applicable on the movie level - for instance we can analyze the same trends on the Top 2 most quoted movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a444cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title, count = np.unique(df.movie, return_counts=True)\n",
    "count_sorted, title = np.array(list(zip(*sorted(zip(count, title), reverse=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"sentiment_VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(-1,-0.5), BERT_bound=(0,1), \n",
    "                  movie=title[0])\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"sentiment_VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(-1, 0.75), second_bound=(0.5, 1),\n",
    "                     movie=title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"sentiment_VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(0.5,1), BERT_bound=(-1,0), \n",
    "                  movie=title[0])\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"sentiment_VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(0.5, 1), second_bound=(0, 0.5),\n",
    "                     movie=title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"sentiment_VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(-1,-0.5), BERT_bound=(0,1), \n",
    "                  movie=title[1])\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"sentiment_VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(-1, 0.75), second_bound=(0.5, 1),\n",
    "                     movie=title[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conditional attribute can eventually be changed to the df.sentiment_AFINN or any other attribute with same data structure\n",
    "plotWithCondition(df, conditional_attribute=\"sentiment_VADER\", second_attribute=\"positive_BERT\",\n",
    "                  cond_bound=(0.5,1), BERT_bound=(-1,0), \n",
    "                  movie=title[1])\n",
    "\n",
    "printConditionQuotes(df, first_attribute=\"sentiment_VADER\",\n",
    "                     second_attribute=\"positive_BERT\", \n",
    "                     N=5, first_bound=(0.5, 1), second_bound=(0, 0.5),\n",
    "                     movie=title[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836e1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6222ab90",
   "metadata": {},
   "source": [
    "Having created the sentiment score in preprocessing, we now just check for the two quotes with minimum (i.e. most negative) and maximum (i.e. most positive) sentiment according to the VADER and BERT sentiment lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ce9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quote with minimum VADER sentiment:\\n\")\n",
    "print(df.loc[VADER_scores.argmin()].quotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d797a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quote with maximum VADER sentiment:\\n\")\n",
    "print(df.loc[VADER_scores.argmax()].quotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quote with minimum BERT sentiment:\\n\")\n",
    "print(df.loc[posBERT_scores.argmin()].quotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quote with maximuma BERT sentiment:\\n\")\n",
    "print(df.loc[posBERT_scores.argmax()].quotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689e461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
